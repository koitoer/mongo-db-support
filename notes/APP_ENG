Write concerns
Write process and Journal
1  -- false  -- Fast, but small window of vulnerability
1  -- true   -- Slow and vulnerability is remove
0  -- T/F    -- Unacknowledge response.

Network errors occurs.
At TCP network connection between server and cliente
MongoDB servers terminate between receiving the write and responding
Fail of the ack in the communication.

Replication is made by three mongo servers, there is one primary and secondaries nodes
Replica set election is being made in the different types of node.
Regular type node --   Has the data an can become primary, is a primary or secondary
Arbitrer            -- Voting, can participate in elections, but no data on it
Delayed/Regular     -- It is a disaster recovery node, some hours before the othere node, but NOT set as primary p=0
Hidden              -- Such use as analysis. never the primary and p=0, can participate in elections, can not be primary

Write consistency
It is only a single in a replicate set, the writes go to the primary and read can go to secondaries.
Strong consistency and avoid steal data if you R/W from the primary
Replication is asynchronous so read from another nodes could be a problem, but will scale the system
In the case of failover, you wont be able to perform any write
Eventual consistency-- Sometimes you have consistency, this is not in the default option in mongo

Create a replica set.
mongod --replSet rs1 --logpath "1.log" --dbPath /data/rs1 --port 27017 --fork
mongod --replSet rs1 --logpath "2.log" --dbPath /data/rs2 --port 27018 --fork
mongod --replSet rs1 --logpath "3.log" --dbPath /data/rs3 --port 27019 --fork

mongoshell $
config = { _id:"rs1" , members [{_id: , host , priority, slaveDelay },{_id: , host }]}
rs.initiate(config)
rs.status()  <-- Give status of replica set
rs.slaveOk() <-- OK to read from secondary !!!
rs.stepDown() <- Stop the process

oplog is reading from the primary, and apply the operations to the secondary, secondaries also can read from another
    secondary nodes
    use local
    show collections
    db.oplog.rs.find()  --- optimeDate

    oplog longest during the time that we suppose to communicate, also the size of the file.
    If is possible to do upgrades, in different mixed storage types, as wiredTiger.

Primary election
Need to have more than the half votes of the replica set 

Replication supports mixed-mode storage engines. For examples, a mmapv1 primary and wiredTiger secondary.
A copy of the oplog is kept on both the primary and secondary servers.
The oplog is implemented as a capped collection.
If the oplog is looped you need to copy the entire dataset from the primary.

In some scenario when the primary goes down and have some writes that the othes node does not have, the failover
can result in a Rollback, if the primary goes down and not replicate the informations to the nodes. 
The rollback statements are saved if you want to apply them manually in the future.
To avoid the previous scneario you could do w=majority, that ensure the majority of the nodes have the data before move on

--Write concern in replicaset
w(number of nodes ack the write)        j                                       wtimeout (time ack from secondary)
1                                       1 (only waiting in the primary)               how long you are going to wait
2
3
majority <--- is a value when you use replica set.
Connection, collection or replicaset


Read preference, by default is over the primary node
You can read from the secondary with read preference, 
    Primary (DEF), read from primary 
    Primary Prefered, primary but if not Im fine with SEC
    Secondary (SEC only) - EVENTUALLY consistent read, rotate read of the secondary, not primary
    Secondary Prefered (SEC y PRM), 
    Nearest (15 ms)
                read_preference = ReadPreference.SECONDARY

You may not read what you previously wrote to MongoDB on a secondary because it will lag behind by some amount.
If your write traffic is great enough, and your secondary is less powerful than the primary, you may overwhelm the secondary, 
            which must process all the writes as well as the reads. Replication lag can result.
If the secondary hardware has insufficient memory to keep the read working set in memory, 
            directing reads to it will likely slow it down.     
    
    
Implications of replication.
. Seeds Lists -- Drivers need to know at least one member of the replicaSet
. Write Concern -- Wait for the ACK in w/j parameters, time the write takes to replicate.
. Read preferences -- What option you will chose, and have support for steal data if that makes the case
. Errors can happen -- Network errors, checking for exception in R/W, failovers, unique key constrants.


BUILDING SHARDING ENVIRONMENTS.  -------------------------------
Auto scalability, handle scaling out.
Collection wil be splitted in different mongo instances, for large collections
Hash based sharding or range based approach.
Shards also have replicaSet, 3 host on a shard, queries get distributed via the mongos
mongos is a router that keep the distribution of the collection in the sharding, we need to have range based approach with shard key
chunks lives in a particular shard, mongos talk with replicaSet and scatter the request , could be more than one mongoS is stateless
If you define a shar key all documents in the collection must have the shard key
Collections not shared are stored in Shard0
Once you have shard and replica you dont connect anymore to the mongo instances, you use the mongos
3 REPLICAset with 3 servers with 3 config servers
Config servers tell us how the collection is distribuited in the shards
Config servers makes two-phase commits in the write of the shards.
Mongos talk to config servers

Building a sharding environment.
Mongos 27017
repicaSet0
repicaSet1
repicaSet2
config0
config1
config2
mongos -- maintain connection with the shards

         CHUNK
Range bash --- Hash based
0...100 S1      hashing algorithm to the shard key
100.199 S2

Implications of sharding
Every document now, include the shard key
Shard key is immutable
Index that start with the shard key, could be a multiple index.
No shard key, ... seak in all the shards
No unique key, unless is part of the shard key, indexes are in each shard, so unique dont work as index dont know the entire collection

What index would be required to allow MongoDB to shard on zip code?
An index on zip or a non-multi-key index that starts with zip. 


Sharding+replication
Are always done together, mongos could have acces sto primaries and secondaries.
Stil the concern of th write concern and j,w, and w_timeout
We can have more than one mongos process but drivers shoudl support that. This be made due to failover in that part.
If you have several mongos, in case of failure the driver will be in charge the redundancy,

Choosing a shard key
1. Sufficient cardinality, sufficient to be a proper key, we can resolve that adding more fields to the shard
   Need to have sufficient cardinality to have spread across the shards.
   We can put a secondary part of the key
2. Avoid hotspotting, multitenant increasing  $minkey --- , $maxkey, a problem if always write to the last shard.
   Most of the dates have this hotspotting problems
   You can not have a key in an array, you need and index, and array have multikey indexes and that is not allowed.


db.adminCommand({enableSharding : true})
db.adminCommand({shardCollection : "collection" , key : {collection_key});

mongos  > sh.status();
