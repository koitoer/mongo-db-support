Write concerns
Write process and Journal
1  -- false  -- Fast, but small window of vulnerability
1  -- true   -- Slow and vulnerability is remove
0  -- T/F    -- Unacknowledge response.

Network errors occurs.
At TCP network connection between server and cliente
MongoDB servers terminate between receiving the write and responding
Fail of the ack in the communication.

Replication is made by three mongo servers, there is one primary and secondaries nodes
Replica set election is being made in the different types of node.
Regular type node --   Has the data an can become primary, is a primary or secondary
Arbitrer            -- Voting, can participate in elections
Delayed/Regular     -- It is a disaster recovery node, some hours before the othere node, but NOT set as primary p=0
Hidden              -- Such use as analysis. never the primary and p=0, can participate in elections

Write consistency
It is only a single in a replicate set, the writes go to the primary and read can go to secondaries.
Strong consistency and avoid steal data if you R/W from the primary
Replication is asynchronous so read from another nodes could be a problem, but will scale the system
In the case of failover, you wont be able to perform any write
Eventual consistency-- Someitmes you have consistency, this is not in the default option in mongo

Create a replica set.
mongod --replSet rs1 --logpath "1.log" --dbPath /data/rs1 --port 27017 --fork
mongod --replSet rs1 --logpath "2.log" --dbPath /data/rs2 --port 27018 --fork
mongod --replSet rs1 --logpath "3.log" --dbPath /data/rs3 --port 27019 --fork

mongoshell $
config = { _id:"rs1" , members [{_id: , host , priority, slaveDelay },{_id: , host }]}
rs.initiate(config)
rs.status()  <-- Give status of replica set
rs.slaveOk() <-- OK to read from secondary
rs.stepDown() <- Stop the process

oplog is reading from the primary, and apply the operations to the secondary, secondaries also can read from another
    secondary nodes
    use local
    show collections
    db.oplog.rs.find()  --- optimeDate

    oplog longest during the time that we suppose to communicate, also the size of the file.
    If is possible to do upgrades, in different mixed storage types, as wiredTiger.

Replication supports mixed-mode storage engines. For examples, a mmapv1 primary and wiredTiger secondary.
A copy of the oplog is kept on both the primary and secondary servers.
The oplog is implemented as a capped collection.

Failover can result in a Rollback, if the primary goes down and not replicate the informations to the nodes.
Read preference, by default is over the primary node
You can read from the secondary with read preference, Primary (DEF), 
    Primary Prefered, Secondary (SEC only) - eventually consisted read, Secondary Prefere (SEC y PRM), Nearest (15 ms)
    
    read_preference = ReadPreference.SECONDARY
    
Implications of replication.
. Seeds Lists -- Drivers need to know at least one member of the replicaSet
. Write Concern -- Wait for the ACK in w/j parameters, time the write takes to replicate.
. Read preferences -- What option you will chose, and have support for steal data if that makes the case
. Errors can happen -- Network errors, checking for exception in R/W


SHARDING
Auto scalability, handle scaling out.
Collection wil be splitted in different mongo instances, for large collections
Shards also have replicaSet, 3 host on a shard, queries get distributed via the mongos
mongos is a router that keep the distribution of the collection in the sharding, we need to have range based approach with shard key
chunks lives in a particular shard, mongos talk with replicaSet and scatter the request , could be more than one mongoS is stateless
If you define a shar key all documents in the collection should have the shard key
Collections not shared are stored in Shard0

Building a sharding environment.
Mongos 27017
repicaSet0
repicaSet1
repicaSet2
config0
config1
config2
mongos

         CHUNK
Range bash -- Hash based


Implications of sharding
Every document now, include the shard key
Shard key is immutable
Index that start with the shard key, could be a multiple index.
No shard key, ... seak in all the shards
No unique key, unless is part of the shard key, indexes are in each shard, so unique dont work as index dont know the entire collection

What index would be required to allow MongoDB to shard on zip code?
An index on zip or a non-multi-key index that starts with zip. 


Sharding+replication
Are always done together, mongos could have acces sto primaries and secondaries.
Stil the concern of th write concern and j,w, and w_timeout
We can have more than one mongos process but drivers shoudl support that. This be made due to failover in that part.

Choosing a shard key
1. Sufficient cardinality, sufficient to be a proper key, we can resolve that adding more fields to the shard
2. Avoid hotspotting, multitenant increasing  $minkey --- , $maxkey, a problem if always write to the last shard.
